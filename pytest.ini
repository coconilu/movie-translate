"""
Pytest configuration for Movie Translate tests
"""

import pytest
import sys
from pathlib import Path

# Add the src directory to Python path
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

# Configure pytest
def pytest_configure(config):
    """Configure pytest"""
    # Add custom markers
    config.addinivalue_line(
        "markers", "slow: marks tests as slow (deselect with '-m \"not slow\"')"
    )
    config.addinivalue_line(
        "markers", "integration: marks tests as integration tests"
    )
    config.addinivalue_line(
        "markers", "unit: marks tests as unit tests"
    )
    config.addinivalue_line(
        "markers", "gui: marks tests as GUI tests"
    )


def pytest_collection_modifyitems(config, items):
    """Modify test collection to add default markers"""
    for item in items:
        # Add unit marker to tests in unit directory
        if "unit" in str(item.fspath):
            item.add_marker(pytest.mark.unit)
        
        # Add integration marker to tests in integration directory
        elif "integration" in str(item.fspath):
            item.add_marker(pytest.mark.integration)
        
        # Add gui marker to GUI tests
        elif "gui" in str(item.fspath) or "ui" in str(item.fspath):
            item.add_marker(pytest.mark.gui)


# Custom command line options
def pytest_addoption(parser):
    """Add custom command line options"""
    parser.addoption(
        "--runslow", action="store_true", default=False, help="run slow tests"
    )
    parser.addoption(
        "--runintegration", action="store_true", default=False, help="run integration tests"
    )
    parser.addoption(
        "--rungui", action="store_true", default=False, help="run GUI tests"
    )


def pytest_runtest_setup(item):
    """Setup for each test"""
    # Skip slow tests unless --runslow is provided
    if "slow" in item.keywords and not item.config.getoption("--runslow"):
        pytest.skip("need --runslow option to run slow tests")
    
    # Skip integration tests unless --runintegration is provided
    if "integration" in item.keywords and not item.config.getoption("--runintegration"):
        pytest.skip("need --runintegration option to run integration tests")
    
    # Skip GUI tests unless --rungui is provided
    if "gui" in item.keywords and not item.config.getoption("--rungui"):
        pytest.skip("need --rungui option to run GUI tests")


# Test fixtures
@pytest.fixture(scope="session")
def test_coverage():
    """Test coverage fixture"""
    import coverage
    cov = coverage.Coverage()
    cov.start()
    yield cov
    cov.stop()
    cov.save()
    cov.report()
    cov.html_report(directory="htmlcov")


# Global test configuration
MIN_PYTHON_VERSION = (3, 8)
MAX_PYTHON_VERSION = (3, 12)

def pytest_sessionstart(session):
    """Session start hook"""
    import sys
    current_version = sys.version_info[:2]
    
    if current_version < MIN_PYTHON_VERSION:
        pytest.exit(f"Python {MIN_PYTHON_VERSION[0]}.{MIN_PYTHON_VERSION[1]}+ required")
    
    if current_version > MAX_PYTHON_VERSION:
        pytest.exit(f"Python {MAX_PYTHON_VERSION[0]}.{MAX_PYTHON_VERSION[1]}- required")


# Performance monitoring
@pytest.fixture(autouse=True)
def monitor_test_performance(request):
    """Monitor test performance"""
    import time
    start_time = time.time()
    
    yield
    
    end_time = time.time()
    duration = end_time - start_time
    
    # Warn about slow tests
    if duration > 5.0:  # 5 seconds
        print(f"\nWARNING: Test {request.node.name} took {duration:.2f} seconds")


# Memory monitoring
@pytest.fixture(autouse=True)
def monitor_test_memory(request):
    """Monitor test memory usage"""
    try:
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        start_memory = process.memory_info().rss
        
        yield
        
        end_memory = process.memory_info().rss
        memory_diff = end_memory - start_memory
        
        # Warn about memory-intensive tests
        if memory_diff > 100 * 1024 * 1024:  # 100MB
            print(f"\nWARNING: Test {request.node.name} used {memory_diff / (1024*1024):.2f}MB additional memory")
            
    except ImportError:
        # psutil not available, skip memory monitoring
        yield


# Test cleanup
@pytest.fixture(autouse=True)
def cleanup_test_environment():
    """Clean up test environment after each test"""
    yield
    
    # Clean up any temporary files or resources
    import tempfile
    import shutil
    
    temp_dir = Path(tempfile.gettempdir())
    test_temp_dirs = [d for d in temp_dir.glob("test_*") if d.is_dir()]
    
    for test_dir in test_temp_dirs:
        try:
            shutil.rmtree(test_dir)
        except PermissionError:
            pass  # Ignore permission errors


# Test data directory
@pytest.fixture(scope="session")
def test_data_dir():
    """Create test data directory"""
    test_dir = Path(__file__).parent / "test_data"
    test_dir.mkdir(exist_ok=True)
    return test_dir


# Test environment variables
@pytest.fixture(autouse=True)
def test_environment():
    """Set up test environment variables"""
    import os
    
    # Store original environment
    original_env = os.environ.copy()
    
    # Set test environment variables
    os.environ["TESTING"] = "1"
    os.environ["DATABASE_URL"] = "sqlite:///:memory:"
    os.environ["LOG_LEVEL"] = "DEBUG"
    
    yield
    
    # Restore original environment
    os.environ.clear()
    os.environ.update(original_env)


# Test logging
@pytest.fixture(autouse=True)
def test_logging():
    """Set up test logging"""
    import logging
    
    # Capture test logs
    class TestLogHandler(logging.Handler):
        def __init__(self):
            super().__init__()
            self.records = []
        
        def emit(self, record):
            self.records.append(record)
    
    handler = TestLogHandler()
    
    # Add handler to all loggers
    for logger_name in logging.Logger.manager.loggerDict:
        logger = logging.getLogger(logger_name)
        logger.addHandler(handler)
    
    yield handler
    
    # Clean up
    for logger_name in logging.Logger.manager.loggerDict:
        logger = logging.getLogger(logger_name)
        logger.removeHandler(handler)


# Test reporting
def pytest_terminal_summary(terminalreporter, exitstatus, config):
    """Custom terminal summary reporting"""
    if exitstatus == 0:
        terminalreporter.write_line("\n‚úÖ All tests passed!", green=True)
    else:
        terminalreporter.write_line("\n‚ùå Some tests failed!", red=True)
    
    # Show test timing summary
    slow_tests = []
    for item in terminalreporter.stats.items():
        if item[0] == "duration":
            for test in item[1]:
                if test.duration > 1.0:  # Slow tests
                    slow_tests.append((test.nodeid, test.duration))
    
    if slow_tests:
        terminalreporter.write_line("\n‚è±Ô∏è  Slowest tests:", yellow=True)
        for test_name, duration in sorted(slow_tests, key=lambda x: x[1], reverse=True)[:5]:
            terminalreporter.write_line(f"   {test_name}: {duration:.2f}s")


# Error handling
def pytest_keyboard_interrupt(excinfo):
    """Handle keyboard interrupt"""
    print("\n\nüõë Tests interrupted by user")
    return True


def pytest_exception_interact(node, call, report):
    """Handle test exceptions"""
    if report.failed:
        print(f"\nüí• Test failed: {node.nodeid}")
        print(f"   Error: {call.excinfo.exconly()}")


# Test parallelization configuration
def pytest_configure(config):
    """Configure parallel test execution"""
    if config.option.numprocesses:
        print(f"üîÄ Running tests in parallel with {config.option.numprocesses} processes")


# Test coverage reporting
def pytest_sessionfinish(session, exitstatus):
    """Generate test coverage report"""
    if exitstatus == 0:
        try:
            import coverage
            cov = coverage.Coverage()
            cov.load()
            cov.report()
            cov.html_report(directory="htmlcov")
            print("\nüìä Coverage report generated in htmlcov/")
        except ImportError:
            pass  # coverage not available


# Performance benchmarks
@pytest.fixture
def benchmark():
    """Benchmark fixture for performance testing"""
    try:
        import pytest_benchmark
        return pytest_benchmark.fixture
    except ImportError:
        # pytest-benchmark not available
        def dummy_benchmark(func, *args, **kwargs):
            return func(*args, **kwargs)
        return dummy_benchmark


# Test data validation
@pytest.fixture
def validate_test_data():
    """Validate test data integrity"""
    def validator(data, schema=None):
        """Validate test data against schema"""
        if schema is None:
            # Basic validation
            assert isinstance(data, (dict, list, str, int, float, bool))
        else:
            # Schema validation would go here
            pass
        return True
    
    return validator


# Test cleanup utilities
@pytest.fixture
def test_cleanup():
    """Test cleanup utilities"""
    def cleanup():
        """Clean up test artifacts"""
        import tempfile
        import shutil
        
        temp_dir = Path(tempfile.gettempdir())
        test_artifacts = list(temp_dir.glob("test_*"))
        
        for artifact in test_artifacts:
            if artifact.is_dir():
                shutil.rmtree(artifact, ignore_errors=True)
            else:
                artifact.unlink(missing_ok=True)
    
    yield cleanup
    cleanup()  # Clean up after test


# Test configuration validation
@pytest.fixture
def validate_test_config():
    """Validate test configuration"""
    import sys
    import os
    
    # Check Python version
    if sys.version_info < (3, 8):
        pytest.exit("Python 3.8+ required")
    
    # Check required environment
    required_env_vars = ["TESTING"]
    for var in required_env_vars:
        if var not in os.environ:
            os.environ[var] = "1"
    
    return True